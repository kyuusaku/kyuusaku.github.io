---
layout: page
title: Research map about convolutional neural networks
permalink: /cnn_map/
---

------

## Network architectures



> **2015**

* **ResNet** 
[paper](http://arxiv.org/abs/1512.03385) 
[project & caffe code](https://github.com/KaimingHe/deep-residual-networks)
[architecture ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006)
[architecture ResNet-101](http://ethereon.github.io/netscope/#/gist/b21e2aae116dc1ac7b50)
[architecture ResNet-152](http://ethereon.github.io/netscope/#/gist/d38f3e6091952b45198b)

> **2014**

* **VGG16**
[paper]()
[architecture](http://ethereon.github.io/netscope/#/preset/vgg-16)

* **GoogLeNet v1**(Inception) 
[paper](http://arxiv.org/abs/1409.4842)
[architecture](http://ethereon.github.io/netscope/#/preset/googlenet)  
Key: Inception module. No. of layers: 22.

* **NIN**:Network In Network 
[paper](http://arxiv.org/abs/1312.4400)
[architecture](http://ethereon.github.io/netscope/#/preset/nin)  
Motivation: Enhancing the local linear model (convolution filter) by multilayer perceptron (\\(1\times1\\) convolution kernel). Rectified linear unit is used as the activation function in the multilayer perceptron.They argue that conventional CNN implicitly makes the assumption that the latent concepts are linearly separable.



> **2012**

* **AlexNet**


------

## Visualization



------

## Optimization




------

## Activation function

> **2016**

* Fast and Accurate Deep Network Learning by **Exponential Linear Units** (ELUs) 
[paper](http://arxiv.org/abs/1511.07289)  


* **BReLU** - Adjustable Bounded Rectifiers: Towards Deep Binary Representations 
[paper](http://arxiv.org/abs/1511.06201)  


> **2015**

* **Adaptive activation function**: Learning Activation Functions to Improve Deep Neural Networks 
[paper](http://arxiv.org/abs/1412.6830)  
aaa


------

## Applications

